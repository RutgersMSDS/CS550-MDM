{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "3.6.9 |Anaconda, Inc.| (default, Jul 30 2019, 19:07:31) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "from surprise import SVD\n",
    "from surprise import NMF\n",
    "from surprise import SVDpp\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "from surprise import NormalPredictor\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8686\n",
      "MAE:  0.6683\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.668271009683475"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame = pd.read_csv(\"res/sample/ratings.csv\")\n",
    "reader = Reader(rating_scale=(1, data_frame.shape[0]))\n",
    "data = Dataset.load_from_df(data_frame[['userId','movieId','rating']], reader)\n",
    "trainset, testset = train_test_split(data, test_size=.20)\n",
    "\n",
    "\n",
    "algo = SVD()\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)\n",
    "\n",
    "# Then compute MAE\n",
    "accuracy.mae(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9541787535114232"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ndcg(y_true, y_pred, k=None, powered=False):\n",
    "    def dcg(scores, k=None, powered=False):\n",
    "        if k is None:\n",
    "            k = scores.shape[0]\n",
    "        if not powered:\n",
    "            ret = scores[0]\n",
    "            for i in range(1, k):\n",
    "                ret += scores[i] / np.log2(i + 1)\n",
    "            return ret\n",
    "        else:\n",
    "            ret = 0\n",
    "            for i in range(k):\n",
    "                ret += (2 ** scores[i] - 1) / np.log2(i + 2)\n",
    "            return ret\n",
    "    \n",
    "    ideal_sorted_scores = np.sort(y_true)[::-1]\n",
    "    ideal_dcg_score = dcg(ideal_sorted_scores, k=k, powered=powered)\n",
    "    \n",
    "    pred_sorted_ind = np.argsort(y_pred)[::-1]\n",
    "    pred_sorted_scores = y_true[pred_sorted_ind]\n",
    "    dcg_score = dcg(pred_sorted_scores, k=k, powered=powered)\n",
    "    \n",
    "    return dcg_score / ideal_dcg_score\n",
    "\n",
    "def ndcg1(y_true, y_pred, k=None):\n",
    "    return ndcg(y_true, y_pred, k=k, powered=False)\n",
    "\n",
    "def ndcg2(y_true, y_pred, k=None):\n",
    "    return ndcg(y_true, y_pred, k=k, powered=True)\n",
    "\n",
    "ndcg_list=[]\n",
    "for uid in top_n:\n",
    "    \n",
    "    for i in users_true[uid]:\n",
    "        y_true=np.asarray(i)#.reshape(-1,1)\n",
    "    for i in users_est[uid]:\n",
    "        y_pred=np.asarray(i)#.reshape(-1,1)\n",
    "        ndcg_list.append(ndcg1(y_true, y_pred, k=None))\n",
    "      \n",
    "\n",
    "ndcg_list = [i for i in ndcg_list if str(i) != 'nan']\n",
    "mean(ndcg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est, true_r))\n",
    "    #uid:[(iid,est),(iid,est)]\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings#[:n]\n",
    "\n",
    "    return top_n\n",
    "\n",
    "\n",
    "top_n = get_top_n(predictions, n=10)\n",
    "#print(top_n)\n",
    "\n",
    "\n",
    "users_est = defaultdict(list)\n",
    "users_true=defaultdict(list)\n",
    "\n",
    "for uid, user_ratings in top_n.items():\n",
    "    users_est[uid].append([est for (_, est,_) in user_ratings])\n",
    "    users_true[uid].append([true_r for (_,_,true_r) in user_ratings])\n",
    "\n",
    "\n",
    "#print (users_true)\n",
    "#print (users_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark in Python 3",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
