{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createToken(my_string):\n",
    "    return re.findall('[\\w\\-]+', my_string.lower())\n",
    "\n",
    "\n",
    "def tokenize_movies(movies):\n",
    "\n",
    "    movies['tokens'] = [createToken(genre) for genre in movies['genres']]\n",
    "\n",
    "    return movies\n",
    "\n",
    "\n",
    "def CreateFeature(movies):\n",
    "    vocabulary = {movie_tokens:index for index, movie_tokens in enumerate(sorted(np.unique(np.concatenate(movies.tokens))))}\n",
    "    df = defaultdict(int)\n",
    "    for movie_genre in movies.tokens:\n",
    "        for genre in vocabulary:\n",
    "            if genre in movie_genre:\n",
    "                df[genre]+=1\n",
    "\n",
    "    all_csr = []\n",
    "    for index, movie in enumerate(movies.tokens):\n",
    "        colmn, data, row = [], [], []\n",
    "        tf = Counter(movie)    \n",
    "        max_k = tf.most_common(1)[0][1]\n",
    "        for genre, freq in tf.items():\n",
    "            if genre in vocabulary:\n",
    "                colmn.append(vocabulary[genre])\n",
    "                data.append((freq/max_k)*math.log10(len(movies)/df[genre])) # tf-idf\n",
    "                X = csr_matrix((np.asarray(data), (np.zeros(shape=(len(data))), np.asarray(colmn))), shape=(1, len(vocabulary)))\n",
    "\n",
    "        all_csr.append(X)\n",
    "\n",
    "    movies['features'] = all_csr\n",
    "\n",
    "\n",
    "    return movies, vocabulary\n",
    "\n",
    "def Find_CosineSim(a, b):\n",
    "    a = a.toarray()\n",
    "    b = b.toarray()\n",
    "    return (np.dot(a,b.T)) / (np.sqrt(np.sum(np.square(a))) * np.sqrt(np.sum(np.square(b))))\n",
    "\n",
    "\n",
    "\n",
    "def make_predictions(movies, ratings_train, ratings_test):\n",
    "    predictions = []\n",
    "    for test_userid, test_movieid in zip(ratings_test.userId, ratings_test.movieId):\n",
    "        weight_ratings = []\n",
    "        weights = []\n",
    "        target_user_ratings = []\n",
    "        for index, train_user in ratings_train.loc[ratings_train.userId == test_userid, 'movieId': 'rating'].iterrows():\n",
    "\n",
    "            cos_sim_weight = Find_CosineSim(movies.loc[movies.movieId == int(train_user.movieId)].features.values[0],\n",
    "                                        movies.loc[movies.movieId == int(test_movieid)].features.values[0])\n",
    "            weight_ratings.append(train_user.rating * cos_sim_weight)\n",
    "            weights.append(cos_sim_weight)\n",
    "            target_user_ratings.append(train_user.rating)\n",
    "\n",
    "\n",
    "        if np.count_nonzero(weights) > 0:\n",
    "            predictions.append(np.sum(weight_ratings)/np.sum(weights))\n",
    "        else:\n",
    "            predictions.append(ratings_train.loc[ratings_train.userId == test_userid, 'rating'].mean())\n",
    "\n",
    "    return np.asarray(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_train = pd.read_csv('train.csv') \n",
    "ratings_test = pd.read_csv('test.csv')\n",
    "movies = pd.read_csv(\"movies.csv\")\n",
    "movies = tokenize_movies(movies)\n",
    "movies, vocabulary = CreateFeature(movies)\n",
    "predictions = make_predictions(movies, ratings_train, ratings_test)\n",
    "predicted_ratings = pd.Series(predictions)\n",
    "ratings_test['predicted_ratings'] = np.array(predicted_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6580660778302934\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(ratings_test['rating'], ratings_test['predicted_ratings']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8514172186463351"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse = sqrt(mean_squared_error(ratings_test['rating'], ratings_test['predicted_ratings']))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
